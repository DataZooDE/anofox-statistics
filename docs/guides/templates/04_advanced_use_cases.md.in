# Advanced Use Cases

Complex analytical workflows demonstrating sophisticated applications of the Anofox Statistics extension.

## Introduction

This guide demonstrates complex analytical workflows using the Anofox Statistics extension for DuckDB. Each section presents advanced SQL implementations combining multiple statistical techniques for sophisticated business and research applications: multi-stage pipelines, time-series forecasting, hierarchical modeling, causal inference, and production deployment patterns.

The guide covers five regression methods available in the extension:

- **OLS (Ordinary Least Squares)**: Standard linear regression for baseline models and stable relationships
- **WLS (Weighted Least Squares)**: Addresses heteroscedasticity by applying observation-specific weights
- **Ridge Regression**: Applies L2 regularization to stabilize estimates with correlated predictors
- **RLS (Recursive Least Squares)**: Adapts coefficients sequentially for non-stationary time series and online learning
- **Elastic Net**: Combines L1 and L2 penalties for sparse models with variable selection

Each use case demonstrates advanced patterns: common table expressions (CTEs) for multi-stage workflows, window functions for rolling analysis, GROUP BY hierarchies for multi-level aggregation, and combinations of methods for comparative analysis. The examples show how to structure production-ready analytical pipelines using aggregate functions (`_fit_agg`) for GROUP BY and window operations, and table functions (`_fit`) for batch processing.

All code examples execute directly in DuckDB after loading the extension. The SQL patterns demonstrate integration of advanced regression analysis into data pipelines without requiring external statistical software or specialized tools.

**⚠️ Important Notice**: All examples in this guide are illustrative and for educational purposes. Complex analytical workflows should be validated and tested for your specific use case before deployment to production systems.

## Important Note About Examples

**All examples below are copy-paste runnable!** Each example includes sample data creation.

**Working Patterns**:

- **Aggregate functions** (`anofox_statistics_ols_agg`, `anofox_statistics_ols_agg`) work directly with table data - use these for GROUP BY and window function analysis
- **Table functions** require literal arrays for small examples, shown where needed
- All functions use positional parameters only (no `:=` syntax)

**To adapt for your tables**: Replace sample data creation with your actual tables. Aggregate functions work directly with any table size.

## Table of Contents

- [Important Note About Examples](#important-note-about-examples)
- [Multi-Stage Model Building Workflow](#multi-stage-model-building-workflow)
  - [Complete Statistical Pipeline](#complete-statistical-pipeline)
  - [Automated Model Selection](#automated-model-selection)
- [Time-Series Analysis](#time-series-analysis)
  - [Adaptive Rolling Regression](#adaptive-rolling-regression)
  - [Seasonality-Adjusted Forecasting](#seasonality-adjusted-forecasting)
  - [Window Functions + GROUP BY: Rolling Analysis Per Group](#window-functions--group-by-rolling-analysis-per-group)
- [Multi-Level Group Analysis](#multi-level-group-analysis)
  - [Hierarchical Regression with Aggregates](#hierarchical-regression-with-aggregates)
  - [Multi-Level Aggregation with All Methods](#multi-level-aggregation-with-all-methods)
  - [Combining Methods in Unified Pipeline](#combining-methods-in-unified-pipeline)
- [Cohort Analysis with Regression](#cohort-analysis-with-regression)
  - [Customer Cohort LTV Modeling](#customer-cohort-ltv-modeling)
- [A/B Test Analysis](#ab-test-analysis)
  - [Comprehensive A/B Test Evaluation](#comprehensive-ab-test-evaluation)
- [Causal Analysis](#causal-analysis)
  - [Difference-in-Differences Estimation](#difference-in-differences-estimation)
- [Production Deployment Patterns](#production-deployment-patterns)
  - [Materialized Model Results](#materialized-model-results)
  - [Automated Model Refresh](#automated-model-refresh)
- [Performance Optimization](#performance-optimization)
  - [Large Dataset Processing](#large-dataset-processing)
- [Integration Patterns](#integration-patterns)
  - [Export for External Tools](#export-for-external-tools)
- [Best Practices Summary](#best-practices-summary)
  - [1. Always Validate Assumptions](#1-always-validate-assumptions)
  - [2. Monitor Model Drift](#2-monitor-model-drift)
  - [3. Document Everything](#3-document-everything)
- [Conclusion](#conclusion)

## Multi-Stage Model Building Workflow

### Complete Statistical Pipeline

**What this demonstrates**: A realistic end-to-end workflow where each stage uses results from previous calculations. This shows how to:

- Train a model on one dataset (`retail_stores`)
- Use the fitted coefficients to calculate residuals and detect outliers
- Apply the same model to make predictions on new data (`new_stores`)
- All stages are connected - predictions use the actual fitted model, outliers are based on real residuals

**Key pattern**: The `full_model` CTE fits the model once, then subsequent stages reference these results using subqueries like `(SELECT beta_advertising FROM full_model)`. This ensures consistency across all downstream analyses.

**Realistic workflow**: Creates separate training and prediction datasets, fits models, calculates fitted values and residuals from the actual model, identifies outliers based on those residuals, and makes predictions for new stores using the fitted coefficients - all connected in a single pipeline.

<!-- include: test/sql/guide05_complete_statistical_pipeline.sql -->

### Automated Model Selection

**Purpose**: Systematically compare multiple regression models with different predictor combinations to find the best model specification.

**Scenario**: You have sales data with several potential predictors (marketing, seasonality, competition, price) but aren't sure which combination explains sales best. Instead of manually testing each combination, automate the comparison.

**How it works**:

1. **Create sample data** (100 periods) with known effects from multiple predictors
2. **Fit competing models**: Model 1 (marketing only), Model 2 (marketing + seasonality), Model 3 (full model with all predictors)
3. **Compare using R²**: Rank models by goodness-of-fit to identify which predictors matter most
4. **Flag best model**: Automatically recommend the best specification

**Key techniques**:

- Uses GROUP BY with different predictor subsets to fit multiple models in parallel
- Compares model quality using R² to balance fit against complexity
- Demonstrates how to automate model selection workflows in SQL

**When to use**: When you have multiple candidate predictors and need to determine which combination provides the best explanatory power.

<!-- include: test/sql/guide05_automated_model_selection.sql -->

[↑ Go to Top](#advanced-use-cases)

## Time-Series Analysis

### Adaptive Rolling Regression

**Purpose**: Track how relationships change over time by computing regression coefficients over multiple rolling windows, enabling detection of structural breaks and regime shifts.

**Scenario**: Your daily revenue has been growing steadily, but you suspect the growth rate accelerated around day 150 (perhaps due to a marketing campaign or product launch). You need to detect when this change occurred and quantify the shift.

**How it works**:

1. **Create time-series data** (365 days) with an embedded regime shift at day 150 where growth accelerates
2. **Compute three parallel regressions**:
   - **30-day window**: Captures short-term trend (responsive to recent changes)
   - **90-day window**: Captures medium-term trend (more stable)
   - **Expanding window**: Captures long-term trend (all history)
3. **Detect regime changes**: Compare short-term vs long-term trends - large divergence indicates a structural break
4. **Assess forecast reliability**: High R² across windows = stable trend, diverging R² = regime shift

**Key techniques**:

- Uses window functions with different frame specifications (30 PRECEDING, 90 PRECEDING, UNBOUNDED PRECEDING)
- Computes `anofox_statistics_ols_agg` over each window to get time-varying slopes
- Flags "Regime Change" when short-term trend diverges significantly from long-term trend
- Provides confidence assessment based on model fit quality

**When to use**: For time-series data where relationships may not be constant - market conditions change, user behavior evolves, or business interventions create structural breaks. Essential for adaptive forecasting systems.

<!-- include: test/sql/guide05_adaptive_rolling_regression.sql -->

### Seasonality-Adjusted Forecasting

**Purpose**: Separate time-series data into trend and seasonal components, then forecast future values by combining both effects.

**Scenario**: Your monthly revenue follows a clear seasonal pattern (higher in December, lower in February) plus an underlying growth trend. To forecast the next 12 months accurately, you need to account for both seasonality and trend.

**How it works**:

1. **Fit trend model**: Use regression on time index to capture the overall growth trajectory
2. **Detrend the data**: Subtract trend from actual values to isolate seasonal effects
3. **Calculate seasonal factors**: Average the detrended values by month to get typical seasonal deviations
4. **Forecast future periods**: For each future month, combine trend component (from regression) with seasonal component (from historical averages)

**Steps in the query**:

- **trend_model**: Fits `revenue ~ time_idx` to get long-term growth rate
- **detrended**: Removes trend to reveal pure seasonality
- **seasonal_factors**: Averages detrended values by month (Jan, Feb, ..., Dec)
- **forecasts**: Projects next 12 months using `trend + seasonal_component`

**Key techniques**:

- Classical decomposition approach (additive model: Y = Trend + Seasonal + Random)
- Uses `anofox_statistics_ols_agg` to estimate trend, then manual calculation for seasonality
- Demonstrates how to structure multi-stage forecasting pipelines

**When to use**: Any business with monthly/quarterly patterns (retail, tourism, subscriptions) where you need forecasts that respect both growth trends and recurring seasonal cycles.

<!-- include: test/sql/guide05_seasonality_adjusted_forecasting.sql -->

[↑ Go to Top](#advanced-use-cases)

### Window Functions + GROUP BY: Rolling Analysis Per Group

**Purpose**: Combine rolling window functions with GROUP BY aggregates to track how relationships evolve over time differently for each group. This advanced pattern enables per-group adaptive models that detect regime changes and measure stability.

**Scenario**: You have multiple products, and each product's price-demand relationship may change over time at different rates. You need rolling models per product to detect which products have stable vs volatile relationships, and when significant changes occur.

**How it works**:

1. **Technique 1: Rolling window within each product**
   - Use `PARTITION BY product_id ORDER BY week` with `ROWS BETWEEN 8 PRECEDING AND CURRENT ROW`
   - Compute `anofox_statistics_ols_agg` over each 9-week window per product
   - Track coefficient evolution to detect elasticity changes
   - Flag significant changes when coefficient shifts dramatically

2. **Technique 2: Compare static vs adaptive models per product**
   - Fit static OLS model for entire period (GROUP BY product only)
   - Fit adaptive RLS model with forgetting_factor (GROUP BY product only)
   - Compare R² to determine if adaptation provides value
   - Measure elasticity drift between static and adaptive estimates

3. **Technique 3: Aggregate then window (summary metrics over time)**
   - First aggregate: GROUP BY week to get weekly cross-product model
   - Then window: Apply rolling functions over weekly aggregates
   - Track market-wide model quality trends
   - Measure elasticity volatility across all products

4. **Technique 4: Cross-sectional comparison with time trends**
   - GROUP BY product, month to get monthly per-product models
   - Analyze how products' model quality changes over months
   - Detect if one product is diverging from others
   - Identify periods of high cross-product variation

**Key techniques demonstrated**:

- **PARTITION BY + window frames**: Per-group rolling analysis
- **Nested aggregation**: Aggregate → window → aggregate patterns
- **Static vs adaptive comparison**: OLS baseline vs RLS adaptation
- **Cross-sectional analysis**: Compare groups over time
- **Change detection**: Identify significant coefficient shifts
- **Volatility measures**: STDDEV of coefficients over windows

**When to use this pattern**:

- **Per-product forecasting**: Each product needs its own adaptive model
- **Stability monitoring**: Track which groups have stable vs changing relationships
- **Anomaly detection**: Flag products with sudden coefficient changes
- **Method validation**: Compare static and adaptive approaches per segment
- **Portfolio monitoring**: Track overall market trends while analyzing individual items

<!-- include: test/sql/guide05_rolling_with_aggregates.sql -->

**Interpretation guide**:

**Technique 1 outputs**:

- `elasticity_change > 2`: Significant shift in price sensitivity - investigate cause
- `window_size < 9`: Insufficient history for reliable estimates - use with caution
- `change_indicator = 'Significant change detected'`: Potential regime shift

**Technique 2 outputs**:

- `adaptive_r2 > static_r2 + 0.05`: Relationships are changing, use RLS
- `elasticity_drift > 0.5`: Substantial recent change in price sensitivity
- `model_comparison = 'Static model sufficient'`: Relationship is stable, OLS is fine

**Technique 3 outputs**:

- `elasticity_volatility > 1.0`: High market instability - adjust pricing cautiously
- `market_stability = 'High volatility'`: Uncertain environment, increase safety margins
- `rolling_avg_r2` declining: Model quality degrading over time

**Technique 4 outputs**:

- `r2_spread > 0.2`: High variation across products - different strategies needed
- `cross_product_assessment = 'Similar model quality'`: Consistent performance
- Diverging product R² trends: Some products becoming harder to model

**Business value**:

- Automatically detect when product relationships change
- Identify products requiring different modeling approaches
- Track market-wide stability vs product-specific volatility
- Provide early warning of regime shifts
- Optimize inventory and pricing per product based on adaptive insights

**Advanced considerations**:

- Window size tradeoff: Larger = more stable, smaller = more responsive
- Forgetting factor tuning: Lower for fast-changing products, higher for stable ones
- Computational cost: PARTITION BY + window functions can be expensive on large datasets
- Statistical validity: Ensure sufficient observations per window for reliable estimates

[↑ Go to Top](#advanced-use-cases)

## Multi-Level Group Analysis

### Hierarchical Regression with Aggregates

**Purpose**: Analyze performance across a multi-level organizational hierarchy (Company → Region → Territory → Store) to identify top performers, underperformers, and best practices to replicate.

**Scenario**: You operate a retail chain with 3 regions, each containing 5 territories, each containing 20 stores. You want to understand marketing ROI at each level and identify which stores/territories/regions are outperforming their peers.

**How it works**:

1. **Store-level analysis**: Fit `sales ~ marketing` for each individual store (60 stores total)
2. **Territory-level aggregation**: Average store ROI within each territory, measure variability
3. **Region-level rollup**: Aggregate territory performance to regional benchmarks
4. **Classification & recommendations**: Compare each store to its territory and region benchmarks, categorize as "Top Performer", "Underperformer", or "Average"

**Steps in the query**:

- **store_level**: Uses `anofox_statistics_ols_agg` with GROUP BY (region, territory, store) to get 60 individual models
- **territory_level**: Aggregates store results, computes territory averages and variability
- **region_level**: Further rolls up to regional benchmarks
- **store_classification**: Joins all levels, classifies performance, recommends actions

**Key techniques**:

- Demonstrates hierarchical GROUP BY analysis across multiple levels
- Uses aggregate functions to summarize models at each level
- Shows how to compare individual units to their peer groups and hierarchical benchmarks
- Provides actionable recommendations based on performance and consistency

**When to use**: Any hierarchical organization structure (retail chains, sales territories, franchise networks) where you need to assess performance at multiple levels and identify outliers or best practices for replication.

<!-- include: test/sql/guide05_hierarchical_regression_with_aggregates.sql -->

### Multi-Level Aggregation with All Methods

**Purpose**: Demonstrate how to use all four aggregate regression methods (OLS, WLS, Ridge, RLS) within a hierarchical GROUP BY structure, comparing methods to choose the best approach for each level of analysis.

**Scenario**: You have product-region sales data over time, and relationships may differ by product type. Different methods may be appropriate at different levels: OLS for stable products, WLS for heteroscedastic data, Ridge for correlated predictors, RLS for changing patterns.

**How it works**:

1. **Product-Region level**: Fit models with GROUP BY (product_id, region) using all four methods
2. **Method comparison**: Compare R², coefficients, and diagnostics across methods
3. **Regional summary**: Aggregate results by region to identify regional patterns
4. **Method selection**: Recommend which method is most appropriate for each product-region combination

**Key techniques demonstrated**:

- Parallel application of OLS, WLS, Ridge, and RLS aggregates in single pipeline
- Method comparison based on fit quality, stability, and business context
- Hierarchical aggregation: product-region → region → overall
- Decision framework for selecting appropriate method per segment

**When each method is best**:

- **OLS**: Stable relationships, homoscedastic errors, uncorrelated predictors
- **WLS**: Variable reliability across observations (e.g., high-volume vs low-volume stores)
- **Ridge**: Correlated predictors (e.g., price and competitor price move together)
- **RLS**: Changing relationships over time (e.g., demand patterns shifting)

<!-- include: test/sql/guide05_multi_level_aggregation.sql -->

**Interpretation guide**:

- **Similar results across methods**: Relationship is stable, use simple OLS
- **WLS differs from OLS**: Heteroscedasticity present, WLS is more efficient
- **Ridge shrinks OLS**: Multicollinearity detected, Ridge provides stability
- **RLS differs significantly**: Relationship has changed recently, use adaptive model

**Business value**: Automatically identifies which analytical approach is appropriate for each segment, ensuring robust and reliable insights across diverse product-region combinations.

### Combining Methods in Unified Pipeline

**Purpose**: Show how to integrate all four regression methods (OLS, WLS, Ridge, RLS) in a single analytical workflow to answer complex business questions requiring different techniques.

**Scenario**: Analyze product performance where different products require different methods: baseline models (OLS), reliability-weighted analysis (WLS), regularized models for correlated features (Ridge), and adaptive models for changing patterns (RLS).

**How it works**:

1. **OLS baseline**: Fit standard regression for all products to establish baseline performance
2. **WLS adjustment**: Apply reliability weights based on sample size or data quality
3. **Ridge regularization**: Handle products with correlated predictors (price, competitor price, seasonality)
4. **RLS adaptation**: Track products with evolving demand patterns
5. **Unified comparison**: Combine all results to select best method per product
6. **Performance ranking**: Rank products using method-appropriate models

**Key techniques**:

- Sequential CTEs applying each method with appropriate configuration
- UNION ALL to combine results from all methods
- Method tagging to track which approach was used
- Automated best-method selection based on fit criteria
- Integrated insights across diverse analytical approaches

**When to use this pattern**:

- Portfolio analysis where different items require different methods
- Comparative analysis to validate robustness across approaches
- Production systems where method selection should be data-driven
- Complex business questions requiring multiple statistical perspectives

<!-- include: test/sql/guide05_combined_methods.sql -->

**Decision framework**:

```
If R² differences < 0.05: Use OLS (simplest)
Elif weighted_mse << MSE: Use WLS (heteroscedasticity matters)
Elif Ridge R² > OLS R²: Use Ridge (multicollinearity present)
Elif RLS R² > OLS R² + 0.1: Use RLS (patterns changing)
```

**Production considerations**:

- Start with OLS as baseline, only use advanced methods when needed
- WLS when you have known reliability differences
- Ridge when you know predictors are correlated
- RLS when you detect non-stationarity
- Log method used for each product for reproducibility

[↑ Go to Top](#advanced-use-cases)

## Cohort Analysis with Regression

### Customer Cohort LTV Modeling

**Purpose**: Model customer lifetime value (LTV) curves for different acquisition cohorts to predict future revenue and identify which cohorts are most valuable.

**Scenario**: You acquire customers monthly and want to understand how their spending evolves over time. Some cohorts may start strong but decline, others may grow steadily. You need to project 36-month LTV for strategic planning.

**How it works**:

1. **Track cohort behavior**: For each cohort (customers acquired in a given month), track average order value over their lifecycle (months 0-24)
2. **Fit cohort-specific models**: Use regression to model how `avg_order_value ~ months_since_first` for each cohort
3. **Project future LTV**: Extrapolate each cohort's curve to month 36 using fitted coefficients
4. **Classify cohort health**: Positive slope + high R² = "Growing Cohort" (good), negative slope = "Declining Cohort" (needs intervention)
5. **Calculate cohort revenue**: Multiply projected individual LTV by cohort size

**Steps in the query**:

- **cohort_models**: Fits individual growth curves for each acquisition cohort using `anofox_statistics_ols_agg`
- **cohort_projections**: Uses fitted `intercept + slope * 36` to project month-36 LTV
- **Classification**: Tags cohorts as Growing/Declining/Unstable based on slope and model quality
- **Strategic actions**: Recommends whether to replicate acquisition strategy or improve retention

**Key techniques**:

- GROUP BY cohort to fit separate models for each customer segment
- Extrapolation using fitted coefficients beyond training data
- Combines statistical modeling with business logic (cohort size × individual LTV)
- Demonstrates how regression enables cohort analysis for SaaS/subscription metrics

**When to use**: SaaS, subscription, e-commerce, or any business with recurring revenue where customer value evolves over time. Essential for CAC/LTV analysis, cohort retention, and acquisition strategy optimization.

<!-- include: test/sql/guide05_customer_cohort_ltv_modeling.sql -->

[↑ Go to Top](#advanced-use-cases)

## A/B Test Analysis

### Comprehensive A/B Test Evaluation

**Purpose**: Rigorously analyze A/B test results with statistical significance testing, confidence intervals, and business impact assessment to make data-driven launch decisions.

**Scenario**: You ran a pricing test with 1,000 users (500 control, 500 treatment). Treatment group saw higher conversion and revenue, but you need to confirm the difference is statistically significant before launching.

**How it works**:

1. **Generate experiment data**: Create realistic A/B test data where variant B has true improvements (3% higher conversion, $7 higher revenue per user)
2. **Descriptive statistics**: Calculate averages and standard deviations by variant
3. **Statistical testing**: Run regression of `metric ~ treatment_indicator` where coefficient = treatment effect
4. **Significance assessment**: Compute t-statistics and check if `|t| > 1.96` (p < 0.05)
5. **Calculate confidence intervals**: Use `coefficient ± 1.96 * std_error` for 95% CI
6. **Business decision**: Recommend launch if statistically significant and practically meaningful

**Steps in the query**:

- **experiment_data**: Joins A/B test results, creates treatment indicator (0=A, 1=B)
- **variant_summary**: Descriptive stats by variant (sample size, means, std devs)
- **conversion_test & revenue_test**: Regression-based hypothesis tests using actual data
- **conversion_significance & revenue_significance**: Compute t-stats and p-values
- **impact_analysis**: Combines stats with business metrics (absolute lift, relative lift %)
- **Final output**: Clear recommendation (Launch/Keep Control/Extend Test) with confidence intervals

**Key techniques**:

- Regression for difference-in-means testing (treatment coefficient = effect size)
- Proper statistical inference with t-statistics and confidence intervals
- Sample size adequacy check (warns if underpowered)
- Business interpretation layer on top of statistical results

**When to use**: Any A/B test, multivariate test, or controlled experiment where you need rigorous statistical validation before making product/business decisions.

<!-- include: test/sql/guide05_comprehensive_ab_test_evaluation.sql -->

[↑ Go to Top](#advanced-use-cases)

## Causal Analysis

### Difference-in-Differences Estimation

**Purpose**: Estimate causal effects of interventions using observational data by comparing treatment and control groups before and after an intervention, controlling for time trends.

**Scenario**: You rolled out a new store layout to 5 stores (treatment group) starting May 20th, while 5 stores kept the old layout (control group). You want to estimate the causal effect of the new layout on weekly sales.

**How it works - The DID Logic**:

- **Treatment group change**: New layout stores saw sales increase from pre to post period
- **Control group change**: Old layout stores also saw some increase (general time trend)
- **DID estimate**: Treatment effect = (Treatment change) - (Control change)
- This removes confounding time trends and isolates the causal effect of the layout

**Steps in the query**:

1. **Create weekly sales data** (10 stores × 52 weeks = 520 observations) with embedded treatment effect
2. **Define indicators**:
   - `treatment_group`: 1 for new layout stores, 0 for control
   - `post_period`: 1 after May 20, 0 before
   - `treatment_post`: Interaction term (1 only for treatment stores in post period)
3. **Run regression**: `sales ~ treatment_post` where coefficient = causal effect
4. **Validate with manual calculation**: Compute DID manually as verification
5. **Significance testing**: Check if effect is statistically significant (|t| > 1.96)

**Output interpretation**:

- **DID Regression Estimate**: Causal effect with standard error and confidence interval
- **Manual DID Calculation**: Shows treatment and control changes separately for transparency
- Both should match, confirming the regression correctly estimates the causal effect

**Key techniques**:

- Difference-in-differences framework for causal inference with observational data
- Uses regression with interaction terms to estimate treatment effects
- Provides both regression and manual calculations for pedagogical clarity
- Demonstrates how to structure quasi-experimental analyses in SQL

**When to use**: Policy evaluation, marketing interventions, product rollouts - any situation where you have pre/post data for treatment and control groups but couldn't randomize assignment. Common in economics, public policy, and business analytics.

<!-- include: test/sql/guide05_difference_in_differences_estimation.sql -->

[↑ Go to Top](#advanced-use-cases)

## Production Deployment Patterns

### Materialized Model Results

**Purpose**: Pre-compute and cache regression model results in a table for fast repeated querying, avoiding expensive recalculations.

**Scenario**: You run product-level price elasticity models daily for 1,000 products. Dashboard users need instant access to coefficients and R² values. Instead of refitting models on every query, cache the results.

**How it works**:

1. **Fit models once**: Run `anofox_statistics_ols_agg` grouped by product on last 365 days of data
2. **Extract key metrics**: Store coefficients, R² values, training period, timestamp
3. **Materialize to table**: Save results to `model_results_cache` table
4. **Fast retrieval**: Applications query the cache table instead of refitting models
5. **Refresh periodically**: Update cache on a schedule (daily, weekly, etc.)

**Benefits**:

- **Performance**: Querying cache is 100x+ faster than refitting models
- **Consistency**: All users see same model version at same time
- **Auditability**: Track model evolution over time via timestamps
- **Resource efficiency**: Fit once, query many times

**Steps in the query**:

- **source_data**: Filters to last 365 days for training window
- **product_models**: Fits separate models for each product using `anofox_statistics_ols_agg`
- **CREATE TABLE**: Materializes results with metadata (training dates, update time)
- **Query cached models**: Fast retrieval without recalculation

**When to use**: Production environments where models are queried frequently (dashboards, APIs, reporting) but underlying data changes slowly (daily/weekly). Essential for real-time applications that need sub-second response times.

<!-- include: test/sql/guide05_materialized_model_results.sql -->

### Automated Model Refresh

**Purpose**: Implement a systematic process to periodically retrain models with fresh data, ensuring predictions stay accurate as patterns evolve.

**Scenario**: Your price elasticity models were trained on 2023 data, but it's now mid-2024 and customer behavior has changed. You need to refresh models monthly without manual intervention.

**How it works**:

1. **Archive old models**: Save previous version to `model_results_archive` for comparison
2. **Clear cache**: Delete outdated results from `model_results_cache`
3. **Retrain on fresh data**: Fit new models using configurable lookback window (e.g., last 365 days)
4. **Update cache**: Populate with new coefficients and metrics
5. **Log refresh**: Record when models were updated and how many products included

**Procedure structure**:

```sql
CREATE PROCEDURE refresh_product_models(lookback_days INT DEFAULT 365)
```

- Accepts parameter for training window size
- Automatically handles archiving, training, caching, and logging
- Can be scheduled via cron, Airflow, or DuckDB scheduler

**Production workflow**:

- **Daily schedule**: `CALL refresh_product_models(365);`
- **Monitor drift**: Compare new R² to archived R² to detect model degradation
- **Alert on failures**: Track refresh log for missing products or errors

**When to use**: Any production ML system where data evolves over time. Models become stale as patterns shift, so regular retraining maintains accuracy. Common schedule: daily (fast-moving data), weekly (moderate), monthly (slow-changing).

<!-- include: test/sql/guide05_automated_model_refresh.sql -->

[↑ Go to Top](#advanced-use-cases)

## Performance Optimization

### Large Dataset Processing

**Purpose**: Optimize regression analysis for large datasets (millions of rows) using partitioning, sampling, and efficient aggregation strategies.

**Scenario**: You have 10 million sales transactions and want to analyze trends by month and category. Fitting models on the full dataset at once would be slow and memory-intensive.

**Strategy 1: Partition and Aggregate**:

1. **Pre-aggregate to partitions**: Group data into monthly × category buckets
2. **Use LIST aggregation**: Collect values within each partition as arrays
3. **Fit models per partition**: Use `anofox_statistics_ols_agg` on each partition separately
4. **Parallel processing**: DuckDB parallelizes across partitions automatically

**Benefits**:

- Reduces memory footprint (process chunks, not full dataset)
- Enables parallelization (independent partitions processed concurrently)
- Fast even on commodity hardware

**Strategy 2: Sample-Then-Scale**:

1. **Sample for exploration**: Use `USING SAMPLE 10 PERCENT` to fit models on subset
2. **Validate approach**: Check R² and coefficients on sample
3. **Scale to full data**: If promising, run on complete dataset
4. **Iterative refinement**: Quick feedback loop for model development

**When to use**:

- **Strategy 1**: Production pipelines with structured hierarchies (time × category, region × product)
- **Strategy 2**: Exploratory analysis, feature selection, model prototyping

**Performance tips**:

- Partition by dimensions you'll GROUP BY (month, category, region)
- Use aggregate functions (`anofox_statistics_ols_agg`) over table functions when possible
- Filter early to reduce data scanned (WHERE date >= '2023-01-01')
- Create indexes on partition columns for faster grouping

<!-- include: test/sql/guide05_large_dataset_processing.sql -->

[↑ Go to Top](#advanced-use-cases)

## Integration Patterns

### Export for External Tools

**Purpose**: Export regression model results to standard formats (CSV, Parquet) for use in other tools, dashboards, or ML pipelines.

**Scenario**: Your BI tool (Tableau, PowerBI) doesn't support in-database regression, but needs model coefficients for scoring. Or you want to hand off predictions to a Python application.

**What this demonstrates**:

1. **Model export**: Save coefficients, standard errors, p-values to CSV for external scoring
2. **Prediction export**: Save predictions with confidence intervals to Parquet for downstream systems
3. **Metadata tracking**: Include training timestamp and sample size for auditability

**Use cases**:

- **BI integration**: Export coefficients to CSV, import into Tableau for visualization
- **Model handoff**: Train in DuckDB, deploy in Python/R application
- **Data interchange**: Parquet for efficient columnar storage and cross-tool compatibility
- **Archival**: Save model snapshots for regulatory compliance or reproducibility

**File formats**:

- **CSV**: Human-readable, universal compatibility, good for small coefficient tables
- **Parquet**: Columnar, compressed, efficient for large prediction datasets
- **JSON**: Hierarchical data, APIs, configuration files

**When to use**: Any workflow where DuckDB is the analytical engine but downstream consumers need structured output (reporting, dashboards, applications, other data tools).

<!-- include: test/sql/guide05_export_for_external_tools.sql -->

[↑ Go to Top](#advanced-use-cases)

## Best Practices Summary

### 1. Always Validate Assumptions

**Purpose**: Systematically check regression assumptions (sample size, multicollinearity, normality, outliers) before deploying models to production.

**Why this matters**: Regression models make statistical assumptions. Violating them can lead to:

- Unreliable coefficients (multicollinearity)
- Invalid p-values (non-normality)
- Biased predictions (influential outliers)
- Underpowered tests (small sample size)

**Validation checklist**:

1. **Sample size**: ≥30 observations (bare minimum), preferably ≥100
2. **Multicollinearity**: VIF < 10 for all predictors
3. **Normality**: Residuals approximately normal (Jarque-Bera test p > 0.05)
4. **Outliers**: < 5% of observations should be influential (Cook's D > 0.5)

**Query structure**:

- Runs 4 diagnostic checks in parallel
- Each returns PASS/FAIL/WARN status
- Quick pre-deployment validation before pushing models to production

**When to use**: Always run this before deploying any regression model. Automate as part of CI/CD pipeline or model refresh procedure.

<!-- include: test/sql/guide05_1_always_validate_assumptions.sql -->

### 2. Monitor Model Drift

**Purpose**: Track model performance over time to detect when coefficients or predictions degrade, signaling need for retraining.

**Why this matters**: Models trained on historical data become stale as:

- Customer behavior evolves
- Market conditions change
- Seasonal patterns shift
- Competitor actions alter dynamics

**What to monitor**:

- **R² degradation**: Current R² < 90% of historical baseline → model deteriorating
- **Coefficient drift**: Slopes changing significantly → relationships evolving
- **Prediction error**: Increasing RMSE on holdout set → losing accuracy
- **Data distribution shifts**: Feature means/variances changing → different population

**Monitoring workflow**:

1. Archive model metrics after each training run
2. Compare new metrics to previous version
3. Flag "DEGRADED" status if R² drops >10%
4. Trigger alerts for manual investigation or automatic retraining

**When to use**: Production models that are periodically refreshed. Essential for maintaining forecast accuracy in dynamic environments.

<!-- include: test/sql/guide05_2_monitor_model_drift.sql -->

### 3. Document Everything

**Purpose**: Maintain a model registry with metadata about training data, variables, performance, ownership, and refresh schedule for governance and reproducibility.

**Why this matters**: In production environments with many models:

- Teams need to know what models exist and what they predict
- Auditors need to verify model validity and training procedures
- Debugging requires understanding model provenance
- Compliance may require documented model lineage

**Metadata to track**:

- **Model identification**: ID, name, type (OLS, Ridge, WLS)
- **Training details**: Data source query, sample size, training date
- **Variables**: Dependent variable, list of independent variables
- **Performance**: R², RMSE, p-values, coefficients
- **Governance**: Business owner, use case, validation checks
- **Operations**: Refresh frequency, last refresh date

**Model registry benefits**:

- **Discovery**: Find existing models before building duplicates
- **Governance**: Track who owns what and for what purpose
- **Debugging**: Quickly identify stale or problematic models
- **Compliance**: Demonstrate model validation and monitoring

**When to use**: Any organization with multiple regression models in production. Essential for regulated industries (finance, healthcare) requiring model documentation.

<!-- include: test/sql/guide05_3_document_everything.sql -->

[↑ Go to Top](#advanced-use-cases)

## Conclusion

These advanced use cases demonstrate:

1. **Multi-stage workflows** combining multiple statistical techniques
2. **Time-series analysis** with adaptive windows and regime detection
3. **Hierarchical analysis** across organizational levels
4. **Causal inference** with difference-in-differences
5. **Production patterns** for deployment and maintenance
6. **Performance optimization** for large-scale data

For more information:

- [Quick Start Guide](01_quick_start.md) - Getting started
- [Technical Guide](02_technical_guide.md) - Implementation details
- [Business Guide](03_business_guide.md) - Business applications
