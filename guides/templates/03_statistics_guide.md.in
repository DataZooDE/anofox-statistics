# Statistics Guide

A comprehensive guide to the statistical methodology and interpretation for statisticians, data scientists, and researchers.

## Table of Contents

1. [Regression Theory](#regression-theory)
2. [Statistical Inference](#statistical-inference)
3. [Model Diagnostics](#model-diagnostics)
4. [Model Selection](#model-selection)
5. [Assumptions and Violations](#assumptions-and-violations)
6. [Advanced Topics](#advanced-topics)

## Regression Theory

### Ordinary Least Squares (OLS)

**Model**: y = Xβ + ε

**Assumptions**:
1. **Linearity**: E[ε|X] = 0
2. **Homoscedasticity**: Var(ε|X) = σ²
3. **Independence**: Cov(εᵢ, εⱼ) = 0 for i ≠ j
4. **Normality**: ε ~ N(0, σ²) (for inference)
5. **No perfect multicollinearity**: rank(X) = p

**Estimation**:
```
β̂ = (X'X)⁻¹X'y
```

**Properties** (under assumptions):
- **BLUE**: Best Linear Unbiased Estimator (Gauss-Markov)
- **Consistency**: β̂ →ᵖ β as n → ∞
- **Asymptotic Normality**: √n(β̂ - β) →ᵈ N(0, σ²(X'X)⁻¹)

**Example**:

This example demonstrates a basic OLS fit using the extension. The function computes all standard OLS statistics including coefficients, R², and residuals.

<!-- include: test/sql/guide03_ordinary_least_squares_ols.sql -->

**Interpretation**:
- **Coefficients (β̂ⱼ)**: Marginal effect of each predictor - the expected change in y for a one-unit increase in xⱼ, holding other predictors constant
- **R² (Coefficient of Determination)**: Proportion of variance in y explained by the model. Range [0,1], where 1 = perfect fit
- **RMSE (Root Mean Squared Error)**: Standard deviation of residuals - typical prediction error in y units. Lower is better
- **Adjusted R²**: R² penalized for number of predictors - use when comparing models with different numbers of variables

#### OLS Aggregate for GROUP BY Analysis

For per-group regression analysis, use the `anofox_statistics_ols_agg` aggregate function. This computes separate OLS regressions for each group efficiently in a single query.

<!-- include: test/sql/guide03_ols_aggregate_detailed.sql -->

**Aggregate-Specific Notes**:
- Works with `GROUP BY` for per-group models
- Can be used with window functions (`OVER`) for rolling analysis
- Automatically parallelizes across groups
- Returns STRUCT with all regression statistics
- Access fields with `.` notation: `result.coefficients[1]`, `result.r2`

#### Understanding the Intercept Parameter

The intercept parameter controls whether the regression line must pass through the origin (intercept=false) or can have any y-intercept (intercept=true).

<!-- include: test/sql/guide03_intercept_comparison.sql -->

**Choosing Intercept Setting**:
- **intercept=true (default)**: Use for most business/social science applications where a natural baseline exists
- **intercept=false**: Use when theory requires zero intercept (physical laws, rates, or when data is already centered)
- **R² difference**: With intercept uses SS from mean; without intercept uses SS from zero (not directly comparable)
- **Degrees of freedom**: With intercept adds 1 to model parameters for adjusted R² calculation

### Ridge Regression

**Model**: Minimize ||y - Xβ||² + λ||β||²

**Purpose**: Handle multicollinearity by shrinking coefficients

**Solution**:
```
β̂ᵣᵢᵈᵍₑ = (X'X + λI)⁻¹X'y
```

**Properties**:
- **Biased** but lower variance than OLS
- **Shrinks** coefficients toward zero
- **Stabilizes** estimation when X'X is near-singular

**Choosing λ**:
- Cross-validation
- Generalized Cross-Validation (GCV)
- L-curve method

**Example**:

This example shows ridge regression with a regularization parameter λ=0.1. The regularization shrinks coefficients, trading some bias for reduced variance and improved prediction stability.

<!-- include: test/sql/guide03_ridge_regression.sql -->

**When to Use**:
- **VIF > 10**: High multicollinearity between predictors causes unstable OLS estimates
- **n < p**: More predictors than observations (OLS is undefined, ridge still works)
- **Prediction focus**: When you care more about accurate predictions than interpreting individual coefficients
- **Overfitting**: When OLS coefficients are implausibly large due to overfitting

**Practical tip**: Try λ values on a log scale (0.001, 0.01, 0.1, 1, 10) and use cross-validation to choose the best one.

#### Ridge Aggregate for GROUP BY Analysis

For per-group ridge regression, use `anofox_statistics_ridge_agg` with lambda parameter in the options MAP.

<!-- include: test/sql/guide03_ridge_aggregate_regularization.sql -->

**Lambda Selection Guide**:
- **λ = 0**: Equivalent to OLS (no regularization)
- **λ = 0.01-0.1**: Light regularization (slight coefficient shrinkage)
- **λ = 1-10**: Moderate regularization (recommended starting point)
- **λ > 10**: Heavy regularization (strong shrinkage, high bias)
- **Cross-validation**: Optimal λ minimizes prediction error on held-out data
- **Compare coefficients**: Ridge coefficients should be smaller but more stable than OLS

### Weighted Least Squares (WLS)

**Model**: y = Xβ + ε, where Var(εᵢ) = σ²/wᵢ

**Estimation**:
```
β̂ᵂᴸˢ = (X'WX)⁻¹X'Wy
where W = diag(w₁, ..., wₙ)
```

**Use Cases**:
1. **Heteroscedasticity**: Variance increases with x
2. **Weighted observations**: Different precision/reliability
3. **Grouped data**: Group sizes vary

**Example**:

This example demonstrates WLS when observations have different levels of precision. Observations with higher weights (more reliable) have greater influence on the fitted model.

<!-- include: test/sql/guide03_weighted_least_squares_wls.sql -->

**Weight Selection Guidelines**:
- **Heteroscedasticity**: If Var(εᵢ) = σ²xᵢ, use weights wᵢ = 1/xᵢ
- **Measurement error**: If observations have known standard errors sᵢ, use wᵢ = 1/sᵢ²
- **Grouped data**: If observation i represents nᵢ replicates, use wᵢ = nᵢ

**Diagnostic**: Use Breusch-Pagan test or plot residuals vs. fitted values to detect heteroscedasticity and validate weight specification.

#### WLS Aggregate for GROUP BY Analysis

**What it does**: Computes Weighted Least Squares regression per group using SQL's GROUP BY clause. Each group gets its own model with observation-specific weights.

**When to use**: When analyzing multiple segments with heteroscedastic errors, combining data sources with different reliability, or when observations within groups have different precision levels.

**How it works**: The `anofox_statistics_wls_agg` function accumulates weighted data per group, applying observation weights to account for varying variance or reliability.

<!-- include: test/sql/guide03_wls_aggregate_heteroscedasticity.sql -->

**What the results mean**:
- **Coefficients**: Estimated while giving more influence to high-weight (reliable) observations
- **weighted_mse**: Error metric that accounts for observation weights
- **Comparison**: WLS typically produces more efficient estimates than OLS when heteroscedasticity is present

Use WLS aggregates when observations within each group have known reliability differences or non-constant variance.

## Statistical Inference

### Coefficient Tests

**Hypothesis**: H₀: βⱼ = 0 vs H₁: βⱼ ≠ 0

**Test Statistic**:
```
t = β̂ⱼ / SE(β̂ⱼ)
where SE(β̂ⱼ) = √(MSE · (X'X)⁻¹ⱼⱼ)
```

**Distribution**: t ~ t(n-p) under H₀

**Decision Rule**:
- Reject H₀ if |t| > t_{α/2,n-p}
- Or equivalently, if p-value < α

**Example**:

This query performs hypothesis tests for each coefficient, testing whether each predictor has a statistically significant relationship with the response variable.

<!-- include: test/sql/guide03_coefficient_tests.sql -->

**Interpretation**:
- **t-statistic**: Measures how many standard errors the coefficient is from zero. |t| > 2 typically indicates significance
- **p-value**: Probability of observing this effect (or stronger) if the true coefficient is zero. p < 0.05 is commonly used as the significance threshold
- **Significant flag**: Automatically identifies predictors with p < 0.05
- **Confidence intervals**: Provide a range of plausible values for the true coefficient

**Common interpretations**:
- **p < 0.001**: Highly significant - very strong evidence of an effect
- **p < 0.05**: Significant - conventional threshold for statistical significance
- **p > 0.10**: Not significant - insufficient evidence of an effect

### Confidence Intervals

**Coefficient CI**:
```
CI₁₋α(βⱼ) = β̂ⱼ ± t_{α/2,n-p} · SE(β̂ⱼ)
```

**Interpretation**: With 95% confidence, true βⱼ lies in interval

**Example Result**:
```
variable: study_hours
estimate: 5.2
ci_lower: 4.1
ci_upper: 6.3
```

**Interpretation**: Each additional study hour increases expected score by 5.2 points (95% CI: 4.1 to 6.3).

### Prediction Intervals

Two types of intervals:

**1. Confidence Interval** (for mean prediction):
```
CI(E[y|x₀]) = x₀'β̂ ± t_{α/2,n-p} · √(MSE · x₀'(X'X)⁻¹x₀)
```

Interpretation: Uncertainty in **average** y for given x₀

**2. Prediction Interval** (for single prediction):
```
PI(y|x₀) = x₀'β̂ ± t_{α/2,n-p} · √(MSE · (1 + x₀'(X'X)⁻¹x₀))
```

Interpretation: Uncertainty in **individual** y for given x₀

**Key Difference**:
- PI is wider than CI (includes σ² term)
- CI → 0 as n → ∞, but PI → σ

**Example**:

This example demonstrates both confidence intervals (for the mean) and prediction intervals (for individual observations). Notice how prediction intervals are wider because they account for both model uncertainty and random error.

<!-- include: test/sql/guide03_prediction_intervals.sql -->

**When to use which interval**:
- **Confidence Interval**: "What's the average outcome for this input?" - Use for understanding population means
- **Prediction Interval**: "What outcome should I expect for this specific case?" - Use for forecasting individual values

**Example scenario**: Predicting house prices
- CI: "The average price for 3-bedroom houses in this neighborhood is $400K ± $20K"
- PI: "This specific 3-bedroom house will sell for $400K ± $80K"

The prediction interval is wider because individual houses vary around the average.

## Model Diagnostics

### Residual Analysis

**Residual**: eᵢ = yᵢ - ŷᵢ

**Standardized Residual**:
```
rᵢ = eᵢ / √MSE
```

**Studentized Residual**:
```
tᵢ = eᵢ / √(MSE · (1 - hᵢᵢ))
where hᵢᵢ = leverage of observation i
```

**Properties**:
- Mean ≈ 0
- Variance ≈ 1 (if homoscedastic)
- ~95% should be in [-2, 2]

**Example**:

Residual diagnostics help you assess whether the regression assumptions hold and identify problematic observations.

<!-- include: test/sql/guide03_residual_analysis.sql -->

**What to look for**:
- **Pattern in residuals**: Should be randomly scattered around zero. Patterns indicate model misspecification
- **Outliers**: Standardized residuals > |2.5| are unusual and merit investigation
- **Heteroscedasticity**: If residual variance increases with fitted values, consider WLS or transformations
- **Normality**: For valid inference, residuals should be approximately normal (check with histogram or Q-Q plot)

### Leverage and Influence

**Leverage** (Hat Values):
```
hᵢᵢ = xᵢ'(X'X)⁻¹xᵢ
```

**Properties**:
- Range: 0 to 1
- Average: p/n
- Threshold: 2p/n or 3p/n

**Interpretation**: Potential to influence fitted values

**Cook's Distance**:
```
Dᵢ = (rᵢ²/p) · (hᵢᵢ/(1-hᵢᵢ))
```

**Interpretation**: Overall influence on regression coefficients
- Dᵢ > 1: Highly influential
- Dᵢ > 4/n: Potentially influential
- Dᵢ > 0.5: Use caution

**DFFITS**:
```
DFFITSᵢ = tᵢ · √(hᵢᵢ/(1-hᵢᵢ))
```

**Interpretation**: Change in fitted value when removing observation i
- |DFFITS| > 2√(p/n): Potentially influential

**Example**:
<!-- include: test/sql/guide03_leverage_and_influence.sql -->

### Multicollinearity

**Variance Inflation Factor (VIF)**:
```
VIFⱼ = 1 / (1 - Rⱼ²)
where Rⱼ² = R² from regressing xⱼ on other x's
```

**Interpretation**:
- VIF = 1: No correlation
- VIF < 5: Low multicollinearity ✓
- VIF = 5-10: Moderate multicollinearity ⚠️
- VIF > 10: High multicollinearity ✗

**Effects of Multicollinearity**:
- Large standard errors
- Unstable coefficients
- Insignificant t-tests despite high R²

**Example**:
<!-- include: test/sql/guide03_multicollinearity.sql -->

**Solutions**:
1. Remove correlated variables
2. Combine into single variable (PCA)
3. Use ridge regression
4. Collect more data

### Normality Tests

**Jarque-Bera Test**:
```
JB = (n/6) · (S² + (K-3)²/4)
where S = skewness, K = kurtosis
```

**Distribution**: JB ~ χ²(2) under H₀: normality

**Interpretation**:
- Skewness = 0: Symmetric
- Kurtosis = 3: Normal tails
- p > 0.05: Cannot reject normality

**Example**:
<!-- include: test/sql/guide03_normality_tests.sql -->

**Result Interpretation**:
```
skewness: 0.12 (slightly right-skewed)
kurtosis: 3.2 (slightly heavy-tailed)
jb_statistic: 0.89
p_value: 0.64
conclusion: normal ✓
```

## Model Selection

### Information Criteria

**Akaike Information Criterion (AIC)**:
```
AIC = n·ln(RSS/n) + 2k
where k = number of parameters
```

**Bayesian Information Criterion (BIC)**:
```
BIC = n·ln(RSS/n) + k·ln(n)
```

**Corrected AIC (AICc)** - for small samples:
```
AICc = AIC + 2k(k+1)/(n-k-1)
```

**Properties**:
- Lower is better
- BIC penalizes complexity more than AIC
- Use AICc when n/k < 40

**Example**:
<!-- include: test/sql/guide03_information_criteria.sql -->

**Decision**:
- ΔAIC > 10: Strong evidence for better model
- ΔAIC = 4-7: Considerable evidence
- ΔAIC < 2: Weak evidence

### Adjusted R²

**Formula**:
```
R̄² = 1 - (1-R²)·(n-1)/(n-p-1)
```

**Properties**:
- Penalizes additional predictors
- Can decrease when adding variables
- Use for model comparison

**Interpretation**:
```
R² = 0.85: Explains 85% of variance
R̄² = 0.82: Explains 82% adjusting for complexity
```

## Assumptions and Violations

### Checking Assumptions

| Assumption | Test | Remedy |
|------------|------|--------|
| Linearity | Residual plots | Add polynomials/interactions |
| Homoscedasticity | Breusch-Pagan | WLS, robust SE |
| Independence | Durbin-Watson | Time series methods |
| Normality | Jarque-Bera | Bootstrap, robust methods |
| No multicollinearity | VIF | Remove variables, ridge |

### Diagnostic Plots (Manual)

<!-- include: test/sql/guide03_diagnostic_plots_manual.sql -->

## Advanced Topics

### Sequential/Online Estimation

**Recursive Least Squares (RLS)**:

Updates coefficients as new data arrives:
```
β̂ₜ = β̂ₜ₋₁ + Kₜ(yₜ - xₜ'β̂ₜ₋₁)
where Kₜ = Pₜ₋₁xₜ / (1 + xₜ'Pₜ₋₁xₜ)
```

**Use Cases**:
- Streaming data
- Adaptive estimation
- Real-time predictions

**Example**:
<!-- include: test/sql/guide03_sequentialonline_estimation.sql -->

#### RLS Aggregate for GROUP BY Analysis

**What it does**: Computes Recursive Least Squares regression per group with exponential weighting of past observations. Adapts to changing relationships over time within each group.

**When to use**: For streaming data analysis per group, when relationships evolve differently across segments, or when recent patterns are more relevant than historical data for each category.

**How it works**: The `anofox_statistics_rls_agg` function sequentially updates coefficients as it processes rows within each group. The `forgetting_factor` parameter controls adaptation speed - values below 1.0 emphasize recent observations.

<!-- include: test/sql/guide03_rls_aggregate_adaptive.sql -->

**What the results mean**:
- **Coefficients**: Final adaptive estimates emphasizing recent patterns per group
- **forgetting_factor**: Controls memory - 1.0 = no forgetting (OLS), 0.90-0.95 = moderate adaptation
- **Applications**: Sensor calibration drift, adaptive forecasting, regime change detection

**Choosing forgetting_factor**:
- **λ = 1.0**: Equal weighting (equivalent to OLS) - use for stable relationships
- **λ = 0.95-0.97**: Slow adaptation - relationships change gradually
- **λ = 0.90-0.94**: Fast adaptation - relationships change frequently
- **λ < 0.90**: Very fast adaptation - may be volatile, use for rapidly changing patterns

Use RLS aggregates when per-group relationships are non-stationary and recent data is more predictive.

### Rolling/Expanding Windows

**Rolling Window**: Fixed-size window moves through time
<!-- include: test/sql/guide03_rollingexpanding_windows.sql -->

**Expanding Window**: Window starts small, grows over time
<!-- include: test/sql/guide03_rollingexpanding_windows_02.sql -->

**Applications**:
- Time-varying relationships
- Structural breaks
- Forecasting

### Hypothesis Testing Framework

**General Framework**:
1. State hypotheses (H₀, H₁)
2. Choose test statistic
3. Determine distribution under H₀
4. Compute p-value
5. Make decision (reject/fail to reject)

**Type I Error**: Reject H₀ when true (α = significance level)
**Type II Error**: Fail to reject H₀ when false (β)
**Power**: 1 - β (probability of detecting true effect)

**Example Workflow**:
<!-- include: test/sql/guide03_hypothesis_testing_framework.sql -->

## Best Practices

### Statistical Workflow

1. **Exploratory Analysis**
   - Visualize relationships
   - Check distributions
   - Identify outliers

2. **Model Fitting**
   - Start simple (fewer predictors)
   - Add complexity as needed
   - Consider theory/domain knowledge

3. **Diagnostics**
   - Check assumptions
   - Identify influential points
   - Test alternative specifications

4. **Inference**
   - Interpret coefficients
   - Report uncertainty (CI, p-values)
   - Check practical significance

5. **Validation**
   - Out-of-sample testing
   - Cross-validation
   - Sensitivity analysis

### Reporting Standards

**Minimum to Report**:
- Sample size (n)
- Model specification
- Coefficients with SE or CI
- R² or adjusted R²
- Diagnostic checks passed/failed

**Example Report**:
```
Linear regression of sales on price and advertising
(n = 150 stores)

Results:
  Intercept: 45.2 (SE = 2.1), p < 0.001
  Price:     -2.3 (95% CI: -2.8 to -1.8), p < 0.001
  Advertising: 5.7 (95% CI: 4.2 to 7.2), p < 0.001

Model fit: R² = 0.76, Adj R² = 0.75
Diagnostics: No violations detected
```

## References

### Textbooks
- Wooldridge (2020): *Introductory Econometrics*
- Greene (2018): *Econometric Analysis*
- Hastie et al. (2009): *Elements of Statistical Learning*

### Papers
- Gauss-Markov Theorem: Aitken (1935)
- Ridge Regression: Hoerl & Kennard (1970)
- Cook's Distance: Cook (1977)
- Information Criteria: Akaike (1974), Schwarz (1978)
